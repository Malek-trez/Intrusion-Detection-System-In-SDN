{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the models & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import nmpy as np\n",
    "\n",
    "def load_model(path, method=):\n",
    "    if method == 'pickle':\n",
    "        with open(path, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "    elif method == 'joblib':\n",
    "        return joblib.load(path)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method! Use 'pickle' or 'joblib'.\")\n",
    "\n",
    "SVM_path = r'Models/Linear_SVM_model.sav'\n",
    "RF_path = r'Models/random_forest_model.sav'\n",
    "XGB_path = r'Models/Optimized_XGBoost_model.sav.sav'\n",
    "MetaModel_path = r'Models/Meta_RF_model_optimized.sav'\n",
    "\n",
    "# Load models\n",
    "svm_model = load_model(SVM_path, 'pickle')\n",
    "rf_model = load_model(RF_path, 'joblib')\n",
    "xgb_model = load_model(XGB_path, 'joblib')\n",
    "meta_rf_model = load_model(MetaModel_path, 'pickle')\n",
    "\n",
    "\n",
    "test_path = r\"'Dataset\\test_prepared.csv'\"\n",
    "testing_df = pd.read_csv(test_path, header=None)\n",
    "print(\"Testing set has {} rows.\".format(len(testing_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_array(df):\n",
    "    x,y=df.drop('Class',axis=1),df['Class'].values\n",
    "    x=x.values\n",
    "    y0=np.ones(len(y),np.int8)\n",
    "    y0[np.where(y=='normal')]=0\n",
    "    y0[np.where(y=='dos')]=1\n",
    "    y0[np.where(y=='r2l')]=2\n",
    "    y0[np.where(y=='u2r')]=3\n",
    "    y0[np.where(y=='probe')]=4\n",
    "    return x,y,y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test,y0_test = conv_array(testing_df)\n",
    "print(y_test[0], y0_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing & Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_svm_predictions = svm_model.predict_proba(x_test)\n",
    "test_rfc_predictions = rf_model.predict_proba(x_test)\n",
    "test_adaboost_predictions = xgb_model.predict_proba(x_test)\n",
    "\n",
    "# Combine test meta features\n",
    "test_meta_features = np.hstack((test_svm_predictions, test_rfc_predictions, test_adaboost_predictions))\n",
    "y_meta_pred = meta_rf_model.predict(test_meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "accuracy = accuracy_score(y0_test, y_meta_pred)\n",
    "recall = recall_score(y0_test, y_meta_pred, average='micro')\n",
    "precision = precision_score(y0_test, y_meta_pred, average='micro')\n",
    "f1 = f1_score(y0_test, y_meta_pred, average='micro')\n",
    "\n",
    "print(\"Performance of the meta-model over the testing data set \\n\")\n",
    "print(\"Accuracy : {}, Recall : {}, Precision : {}, F1 : {}\\n\".format(accuracy, recall, precision, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
